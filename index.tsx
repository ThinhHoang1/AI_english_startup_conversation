/* eslint-disable @typescript-eslint/no-explicit-any */
/**
 * @license
 * SPDX-License-Identifier: Apache-2.0
 */

import { GoogleGenAI, LiveServerMessage, Modality, Session } from '@google/genai';
import { LitElement, css, html } from 'lit';
import { customElement, state } from 'lit/decorators.js';
import { createBlob, decode, decodeAudioData } from './utils';
import './visual-3d';

@customElement('gdm-live-audio')
export class GdmLiveAudio extends LitElement {
  @state() isRecording = false;
  @state() status = '';
  @state() error = '';

  private client!: GoogleGenAI;
  private session!: Session;

  // FIX: h·ªó tr·ª£ c·∫£ AudioContext v√† webkitAudioContext
  private inputAudioContext = new ((window as any).AudioContext || (window as any).webkitAudioContext)({ sampleRate: 16000 });
  private outputAudioContext = new ((window as any).AudioContext || (window as any).webkitAudioContext)({ sampleRate: 24000 });

  @state() inputNode = this.inputAudioContext.createGain();
  @state() outputNode = this.outputAudioContext.createGain();

  private nextStartTime = 0;
  private mediaStream: MediaStream | null = null;
  private sourceNode: MediaStreamAudioSourceNode | null = null;
  private scriptProcessorNode: ScriptProcessorNode | null = null;
  private sources = new Set<AudioBufferSourceNode>();

  static styles = css`
    #status {
      position: absolute;
      bottom: 5vh;
      left: 0;
      right: 0;
      z-index: 10;
      text-align: center;
      color: white;
    }

    .controls {
      z-index: 10;
      position: absolute;
      bottom: 10vh;
      left: 0;
      right: 0;
      display: flex;
      align-items: center;
      justify-content: center;
      flex-direction: column;
      gap: 10px;
    }

    button {
      outline: none;
      border: 1px solid rgba(255, 255, 255, 0.2);
      color: white;
      border-radius: 12px;
      background: rgba(255, 255, 255, 0.1);
      width: 64px;
      height: 64px;
      cursor: pointer;
      font-size: 24px;
      padding: 0;
      margin: 0;
    }

    button:hover {
      background: rgba(255, 255, 255, 0.2);
    }

    button[disabled] {
      opacity: 0.4;
      cursor: not-allowed;
    }
  `;

  constructor() {
    super();
    this.initClient();
  }

  private initAudio() {
    this.nextStartTime = this.outputAudioContext.currentTime;
  }

  private async initClient() {
    this.initAudio();

    this.client = new GoogleGenAI({
      apiKey: import.meta.env.VITE_API_KEY || '',
    });

    this.outputNode.connect(this.outputAudioContext.destination);
    await this.initSession();
  }

  private async initSession() {
    const model = 'gemini-2.5-flash-preview-native-audio-dialog';

    try {
      this.session = await this.client.live.connect({
        model,
        callbacks: {
          onopen: () => this.updateStatus('üîó Connected'),
          onmessage: async (message: LiveServerMessage) => {
            const audio = message.serverContent?.modelTurn?.parts?.[0]?.inlineData;

            if (audio) {
              this.nextStartTime = Math.max(this.nextStartTime, this.outputAudioContext.currentTime);
              const audioBuffer = await decodeAudioData(
                decode(audio.data),
                this.outputAudioContext,
                24000,
                1,
              );

              const source = this.outputAudioContext.createBufferSource();
              source.buffer = audioBuffer;
              source.connect(this.outputNode);
              source.addEventListener('ended', () => this.sources.delete(source));

              source.start(this.nextStartTime);
              this.nextStartTime += audioBuffer.duration;
              this.sources.add(source);
            }

            if (message.serverContent?.interrupted) {
              for (const s of this.sources.values()) {
                s.stop();
                this.sources.delete(s);
              }
              this.nextStartTime = 0;
            }
          },
          onerror: (e: ErrorEvent) => this.updateError(e.message),
          onclose: (e: CloseEvent) => this.updateStatus(`‚ùå Closed: ${e.reason}`),
        },
        config: {
          systemInstruction: `
B·∫°n l√† **C√¥ Emma**, m·ªôt gi√°o vi√™n ti·∫øng Anh ·∫£o th√¢n thi·ªán v√† chuy√™n nghi·ªáp, d·∫°y h·ªçc cho ng∆∞·ªùi Vi·ªát Nam ·ªü m·ªçi tr√¨nh ƒë·ªô.
M·ª•c ti√™u c·ªßa b·∫°n l√† gi√∫p ng∆∞·ªùi h·ªçc hi·ªÉu ng·ªØ ph√°p, t·ª´ v·ª±ng, ph√°t √¢m, v√† ph·∫£n x·∫° giao ti·∫øp ti·∫øng Anh m·ªôt c√°ch t·ª± nhi√™n.
B·∫°n n√≥i chuy·ªán b·∫±ng ti·∫øng Vi·ªát l·∫´n ti·∫øng Anh, t√πy theo tr√¨nh ƒë·ªô c·ªßa h·ªçc vi√™n.
Lu√¥n gi·∫£i th√≠ch r√µ r√†ng, d·ªÖ hi·ªÉu, v√† s·ª≠ d·ª•ng v√≠ d·ª• th·ª±c t·∫ø.
Gi·ªçng ƒëi·ªáu ·∫•m √°p, kh√≠ch l·ªá, v√† l·ªãch s·ª± ‚Äì gi·ªëng nh∆∞ m·ªôt ng∆∞·ªùi th·∫ßy t·∫≠n t√¢m gi√∫p h·ªçc tr√≤ ti·∫øn b·ªô.
          `,
          responseModalities: [Modality.AUDIO],
          speechConfig: {
            voiceConfig: { prebuiltVoiceConfig: { voiceName: 'Zephyr' } },
          },
        },
      });
    } catch (e) {
      console.error(e);
      this.updateError('Kh√¥ng th·ªÉ k·∫øt n·ªëi v·ªõi Google GenAI API.');
    }
  }

  private updateStatus(msg: string) {
    this.status = msg;
  }

  private updateError(msg: string) {
    this.error = msg;
    console.error(msg);
  }

  private async startRecording() {
    if (this.isRecording) return;

    await this.inputAudioContext.resume();
    this.updateStatus('üéôÔ∏è ƒêang xin quy·ªÅn truy c·∫≠p micro...');

    try {
      this.mediaStream = await navigator.mediaDevices.getUserMedia({ audio: true });
      this.sourceNode = this.inputAudioContext.createMediaStreamSource(this.mediaStream);
      this.sourceNode.connect(this.inputNode);

      const bufferSize = 256;
      this.scriptProcessorNode = this.inputAudioContext.createScriptProcessor(bufferSize, 1, 1);

      this.scriptProcessorNode.onaudioprocess = (event) => {
        if (!this.isRecording) return;
        const inputBuffer = event.inputBuffer;
        const pcmData = inputBuffer.getChannelData(0);
        this.session?.sendRealtimeInput({ media: createBlob(pcmData) });
      };

      this.sourceNode.connect(this.scriptProcessorNode);
      this.scriptProcessorNode.connect(this.inputAudioContext.destination);

      this.isRecording = true;
      this.updateStatus('üî¥ ƒêang ghi √¢m...');
    } catch (err: any) {
      this.updateError(`Kh√¥ng th·ªÉ ghi √¢m: ${err.message}`);
      this.stopRecording();
    }
  }

  private stopRecording() {
    if (!this.isRecording) return;

    this.isRecording = false;
    this.updateStatus('‚èπÔ∏è D·ª´ng ghi √¢m');

    if (this.scriptProcessorNode) this.scriptProcessorNode.disconnect();
    if (this.sourceNode) this.sourceNode.disconnect();

    if (this.mediaStream) {
      this.mediaStream.getTracks().forEach((track) => track.stop());
      this.mediaStream = null;
    }
  }

  private reset() {
    this.session?.close();
    this.initSession();
    this.updateStatus('üîÅ Phi√™n m·ªõi ƒë∆∞·ª£c t·∫°o.');
  }

  render() {
    return html`
      <div>
        <div class="controls">
          <button @click=${this.reset} ?disabled=${this.isRecording}>üîÅ</button>
          <button @click=${this.startRecording} ?disabled=${this.isRecording}>üéôÔ∏è</button>
          <button @click=${this.stopRecording} ?disabled=${!this.isRecording}>‚èπÔ∏è</button>
        </div>

        <div id="status">${this.error || this.status}</div>

        <gdm-live-audio-visuals-3d
          .inputNode=${this.inputNode}
          .outputNode=${this.outputNode}>
        </gdm-live-audio-visuals-3d>
      </div>
    `;
  }
}
